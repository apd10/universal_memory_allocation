{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashedEmbeddingBag\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE=\"mean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomNumbers:  tensor([2038074743,  634329019, 1825252241,  871205356,   80759397])\n",
      "Central weight rand_hash val_offset 0\n",
      "HashedEmbeddingBag:  100 16 mode mean hmode rand_hash kmode keymode_hashweight central True key_bits 4 keys_to_use 8 weight_size 100 uma_chunk_size 1\n"
     ]
    }
   ],
   "source": [
    "uma_size = 100\n",
    "n1 = 100\n",
    "m1 = 16\n",
    "_weight = nn.Parameter(torch.randn((uma_size,)).cuda(0), requires_grad=True)\n",
    "e1 = hashedEmbeddingBag.HashedEmbeddingBag(n1, m1, _weight=_weight, val_offset=0, mode=MODE).cuda(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create exact replica in nn.Embedding\n",
    "\n",
    "actual_wt = torch.clone(e1.weight.data)\n",
    "_weight = torch.arange(uma_size,).float().cuda(0)\n",
    "e1.weight.data[:] = _weight\n",
    "indices_all = torch.arange(100).cuda(0)\n",
    "offsets_all = torch.arange(100).cuda(0)\n",
    "IDX = e1(indices_all)\n",
    "e1.weight.data[:] = actual_wt\n",
    "e_orig = torch.nn.EmbeddingBag(n1, m1, mode=MODE).cuda(0)\n",
    "e_orig.weight.data[:,:] = e1(indices_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.arange(25).cuda(0)\n",
    "offsets = torch.arange(4).cuda(0) * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 10, 15], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward comparision\n",
    "orig_output = e_orig(indices, offsets)\n",
    "rz_output = e1(indices, offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward tensor(0., device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Forward\", torch.max(torch.abs(orig_output - rz_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_loss = torch.sum(orig_output)\n",
    "rz_loss = torch.sum(rz_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_loss.backward()\n",
    "rz_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rz_grad = torch.clone(e1.weight.grad.data)\n",
    "actual_grad = torch.clone(e_orig.weight.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward tensor(1.1921e-07, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "actual_wt_grad = torch.zeros_like(e1.weight).cuda(0)\n",
    "actual_wt_grad.scatter_add_(0, IDX.long().reshape(-1), actual_grad.reshape(-1))\n",
    "print(\"Backward\", torch.max(torch.abs(rz_grad - actual_wt_grad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3000, 1.0000, 0.3000, 0.8000, 0.2000, 1.0000, 0.2000, 1.0000, 0.4000,\n",
       "        1.0000, 0.3000, 0.8000, 0.4000, 1.0000, 0.5000, 0.8000, 0.6000, 0.8000,\n",
       "        0.5000, 0.8000, 0.9000, 0.4000, 0.6000, 0.5000, 0.6000, 0.3000, 0.7000,\n",
       "        0.4000, 0.7000, 0.2000, 0.9000, 0.3000, 1.0000, 0.4000, 1.0000, 0.4000,\n",
       "        0.8000, 0.4000, 1.2000, 0.5000, 0.8000, 0.6000, 0.8000, 0.5000, 0.8000,\n",
       "        0.9000, 0.7000, 0.6000, 0.5000, 0.8000, 0.5000, 0.9000, 0.4000, 0.9000,\n",
       "        0.4000, 0.8000, 0.5000, 1.2000, 0.3000, 0.8000, 0.4000, 1.0000, 0.4000,\n",
       "        0.8000, 0.7000, 0.8000, 0.5000, 0.8000, 0.7000, 0.8000, 0.7000, 0.6000,\n",
       "        0.8000, 0.4000, 0.7000, 0.7000, 0.9000, 0.4000, 0.8000, 0.4000, 0.8000,\n",
       "        0.2000, 1.2000, 0.4000, 0.8000, 0.3000, 1.0000, 0.3000, 1.0000, 0.4000,\n",
       "        0.8000, 0.4000, 0.6000, 0.6000, 0.6000, 0.6000, 0.4000, 0.7000, 0.4000,\n",
       "        0.6000], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_wt_grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
